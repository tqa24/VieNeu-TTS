{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü¶ú VieNeu-TTS Fine-tuning Notebook\n",
        "\n",
        "Notebook n√†y t·ªïng h·ª£p to√†n b·ªô code training cho **VieNeu-TTS-0.3B**.  \n",
        "B·∫°n c√≥ th·ªÉ thay sang **VieNeu-TTS** ·ªü ph·∫ßn `training_config` (m·ª•c 6) n·∫øu mu·ªën.\n",
        "\n",
        "Trong qu√° tr√¨nh training, n·∫øu g·∫∑p l·ªói ho·∫∑c c√≥ g√≥p √Ω, vui l√≤ng t·∫°o **issue** tr√™n GitHub:  \n",
        "https://github.com/pnnbao97/VieNeu-TTS  \n",
        "\n",
        "Ho·∫∑c li√™n h·ªá tr·ª±c ti·∫øp v·ªõi t√°c gi·∫£ **Ph·∫°m Nguy·ªÖn Ng·ªçc B·∫£o** qua:\n",
        "- Email: pnnbao@gmail.com  \n",
        "- Facebook: https://www.facebook.com/bao.phamnguyenngoc.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers peft torch datasets librosa soundfile tqdm phonemizer\n",
        "!pip install -q git+https://github.com/Neuphonic/NeuCodec.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!apt install espeak-ng -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß 2. Setup Utils "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pnnbao97/VieNeu-TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def setup_vieneu_tts():\n",
        "    \"\"\"Universal setup for VieNeu-TTS - works on any platform\"\"\"\n",
        "    \n",
        "    # Find VieNeu-TTS\n",
        "    search_paths = [\n",
        "        \"/root/VieNeu-TTS\",\n",
        "        \"/content/VieNeu-TTS\",\n",
        "        \"./VieNeu-TTS\",\n",
        "        \"../VieNeu-TTS\",\n",
        "    ]\n",
        "    \n",
        "    vieneu_path = None\n",
        "    for path in search_paths:\n",
        "        if os.path.exists(path) and os.path.exists(os.path.join(path, \"utils\")):\n",
        "            vieneu_path = os.path.abspath(path)\n",
        "            break\n",
        "    \n",
        "    if not vieneu_path:\n",
        "        raise FileNotFoundError(\n",
        "            \"VieNeu-TTS not found! Clone it:\\n\"\n",
        "            \"  git clone https://github.com/pnnbao97/VieNeu-TTS\"\n",
        "        )\n",
        "    \n",
        "    # Clean and add to path\n",
        "    sys.path = [p for p in sys.path if \"VieNeu-TTS\" not in p]\n",
        "    sys.path.insert(0, vieneu_path)\n",
        "    \n",
        "    print(f\"‚úÖ VieNeu-TTS: {vieneu_path}\")\n",
        "    \n",
        "    from vieneu_utils.normalize_text import VietnameseTTSNormalizer\n",
        "    from vieneu_utils.phonemize_text import phonemize_with_dict\n",
        "    \n",
        "    # Initialize normalizer\n",
        "    normalizer = VietnameseTTSNormalizer()\n",
        "    \n",
        "    # Create wrapper function\n",
        "    def normalize_text(text):\n",
        "        return normalizer.normalize(text)\n",
        "    \n",
        "    print(\"‚úÖ Utils loaded!\")\n",
        "    \n",
        "    return vieneu_path, normalize_text, phonemize_with_dict\n",
        "\n",
        "# Run setup\n",
        "VIENEU_PATH, normalize_text, phonemize_with_dict = setup_vieneu_tts()\n",
        "\n",
        "# ========== TEST ==========\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "    normalized = normalize_text(text)\n",
        "    phonemes = phonemize_with_dict(normalized)\n",
        "    return {\n",
        "        \"original\": text,\n",
        "        \"normalized\": normalized,\n",
        "        \"phonemes\": phonemes\n",
        "    }\n",
        "\n",
        "# Quick test\n",
        "result = preprocess_text(\"T√¥i c√≥ 2.000 m·∫´u audio, gi√° 5.000.000ƒë\")\n",
        "print(f\"\\nüìù Test result:\")\n",
        "for key, val in result.items():\n",
        "    print(f\"  {key:12s}: {val}\")\n",
        "\n",
        "print(\"\\nüéâ Ready to use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• 3. Download Sample Data\n",
        "\n",
        "T·∫£i d·ªØ li·ªáu m·∫´u t·ª´ Hugging Face (ho·∫∑c thay b·∫±ng dataset c·ªßa b·∫°n).  \n",
        "Trong notebook n√†y, ch√∫ng t√¥i s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu m·∫´u:  \n",
        "https://huggingface.co/datasets/pnnbao-ump/ngochuyen_voice  \n",
        "\n",
        "Dataset n√†y ƒë∆∞·ª£c d√πng ƒë·ªÉ training gi·ªçng ƒë·ªçc **Ng·ªçc Huy·ªÅn (Vbee)** v√† **kh√¥ng n·∫±m trong b·ªô VieNeu-TTS-1000h**,  \n",
        "v√¨ v·∫≠y r·∫•t ph√π h·ª£p ƒë·ªÉ l√†m v√≠ d·ª• minh h·ªça cho qu√° tr√¨nh fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import io\n",
        "from datasets import load_dataset, Audio\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "\n",
        "def download_sample_data(output_dir=\"dataset\", num_samples=10):\n",
        "    raw_audio_dir = os.path.join(output_dir, \"raw_audio\")\n",
        "    metadata_path = os.path.join(output_dir, \"metadata.csv\")\n",
        "    \n",
        "    os.makedirs(raw_audio_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"üîÑ ƒêang t·∫£i dataset t·ª´ Hugging Face...\")\n",
        "    dataset = load_dataset(\"pnnbao-ump/ngochuyen_voice\", split=\"train\")\n",
        "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
        "    \n",
        "    print(f\"‚úÖ B·∫Øt ƒë·∫ßu l∆∞u {num_samples} m·∫´u...\")\n",
        "    \n",
        "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "        count = 0\n",
        "        for sample in tqdm(dataset, total=num_samples):\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "            \n",
        "            try:\n",
        "                audio_data = sample[\"audio\"]\n",
        "                audio_bytes = audio_data[\"bytes\"]\n",
        "                audio_array, sampling_rate = sf.read(io.BytesIO(audio_bytes))\n",
        "                \n",
        "                text = sample[\"transcription\"]\n",
        "                original_filename = sample.get(\"file_name\", f\"sample_{count:03d}.wav\")\n",
        "                filename = os.path.basename(original_filename)\n",
        "                \n",
        "                file_path = os.path.join(raw_audio_dir, filename)\n",
        "                sf.write(file_path, audio_array, sampling_rate)\n",
        "                \n",
        "                f.write(f\"{filename}|{text}\\n\")\n",
        "                count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ö†Ô∏è L·ªói m·∫´u {count}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    print(f\"\\nü¶ú Ho√†n t·∫•t! ƒê√£ t·∫°o {count} m·∫´u t·∫°i {output_dir}\")\n",
        "    return metadata_path\n",
        "\n",
        "# Download data (thay ƒë·ªïi num_samples theo nhu c·∫ßu)\n",
        "metadata_path = download_sample_data(output_dir=\"dataset\", num_samples=7000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ 4. Filter Data\n",
        "\n",
        "L·ªçc d·ªØ li·ªáu k√©m ch·∫•t l∆∞·ª£ng (audio h·ªèng, text r√°c, qu√° ng·∫Øn/d√†i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "ACRONYM = re.compile(r\"(?:[a-zA-Z]\\.){2,}\")\n",
        "ACRONYM_NO_PERIOD = re.compile(r\"(?:[A-Z]){2,}\")\n",
        "\n",
        "def text_filter(text: str) -> bool:\n",
        "    if not text: return False\n",
        "    if re.search(r\"\\d\", text): return False\n",
        "    if ACRONYM.search(text) or ACRONYM_NO_PERIOD.search(text): return False\n",
        "    if text[-1] not in \".,?!\": return False\n",
        "    return True\n",
        "\n",
        "def filter_dataset(dataset_dir=\"dataset\"):\n",
        "    metadata_path = os.path.join(dataset_dir, \"metadata.csv\")\n",
        "    cleaned_path = os.path.join(dataset_dir, \"metadata_cleaned.csv\")\n",
        "    raw_audio_dir = os.path.join(dataset_dir, \"raw_audio\")\n",
        "    \n",
        "    if not os.path.exists(metadata_path):\n",
        "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y {metadata_path}\")\n",
        "        return\n",
        "    \n",
        "    print(\"üßπ B·∫Øt ƒë·∫ßu l·ªçc d·ªØ li·ªáu...\")\n",
        "    \n",
        "    valid_samples = []\n",
        "    skipped = {\"audio_not_found\": 0, \"audio_error\": 0, \"duration_out_of_range\": 0, \"text_invalid\": 0}\n",
        "    \n",
        "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    for line in tqdm(lines, desc=\"Filtering\"):\n",
        "        parts = line.strip().split('|')\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "        \n",
        "        filename = parts[0]\n",
        "        text = parts[1]\n",
        "        file_path = os.path.join(raw_audio_dir, filename)\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            skipped[\"audio_not_found\"] += 1\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            info = sf.info(file_path)\n",
        "            duration = info.duration\n",
        "            \n",
        "            if not (3.0 <= duration <= 15.0):\n",
        "                skipped[\"duration_out_of_range\"] += 1\n",
        "                continue\n",
        "        except Exception:\n",
        "            skipped[\"audio_error\"] += 1\n",
        "            continue\n",
        "        \n",
        "        if not text_filter(text):\n",
        "            skipped[\"text_invalid\"] += 1\n",
        "            continue\n",
        "        \n",
        "        valid_samples.append(f\"{filename}|{text}\\n\")\n",
        "    \n",
        "    with open(cleaned_path, 'w', encoding='utf-8') as f:\n",
        "        f.writelines(valid_samples)\n",
        "    \n",
        "    print(f\"\\nü¶ú K·∫æT QU·∫¢ L·ªåC:\")\n",
        "    print(f\"   - T·ªïng: {len(lines)} | H·ª£p l·ªá: {len(valid_samples)} ({len(valid_samples)/len(lines)*100:.1f}%)\")\n",
        "    print(f\"   - Lo·∫°i b·ªè: {sum(skipped.values())} ({skipped})\")\n",
        "    print(f\"‚úÖ ƒê√£ l∆∞u: {cleaned_path}\")\n",
        "    return cleaned_path\n",
        "\n",
        "cleaned_metadata_path = filter_dataset(dataset_dir=\"dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîä 5. Encode Audio to VQ Codes\n",
        "\n",
        "S·ª≠ d·ª•ng NeuCodec ƒë·ªÉ encode audio th√†nh vector quantized codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import librosa\n",
        "from neucodec import NeuCodec\n",
        "import json\n",
        "import random\n",
        "\n",
        "def encode_dataset(dataset_dir=\"dataset\", max_samples=2000):\n",
        "    metadata_path = os.path.join(dataset_dir, \"metadata_cleaned.csv\")\n",
        "    if not os.path.exists(metadata_path):\n",
        "        print(f\"ü¶ú Kh√¥ng t√¨m th·∫•y metadata_cleaned.csv, d√πng metadata.csv...\")\n",
        "        metadata_path = os.path.join(dataset_dir, \"metadata.csv\")\n",
        "    \n",
        "    output_path = os.path.join(dataset_dir, \"metadata_encoded.csv\")\n",
        "    raw_audio_dir = os.path.join(dataset_dir, \"raw_audio\")\n",
        "    \n",
        "    if not os.path.exists(metadata_path):\n",
        "        print(\"ü¶ú Kh√¥ng t√¨m th·∫•y metadata!\")\n",
        "        return\n",
        "    \n",
        "    print(\"ü¶ú ƒêang t·∫£i NeuCodec model...\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    codec = NeuCodec.from_pretrained(\"neuphonic/neucodec\").to(device)\n",
        "    codec.eval()\n",
        "    \n",
        "    print(f\"ü¶ú Encode t·ªëi ƒëa {max_samples} m·∫´u (device: {device})\")\n",
        "    \n",
        "    lines_to_write = []\n",
        "    skipped_count = 0\n",
        "    \n",
        "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    # Shuffle v√† l·∫•y max_samples\n",
        "    random.shuffle(lines)\n",
        "    if len(lines) > max_samples:\n",
        "        lines = lines[:max_samples]\n",
        "    \n",
        "    for line in tqdm(lines, desc=\"Encoding\"):\n",
        "        parts = line.strip().split('|')\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "        \n",
        "        filename = parts[0]\n",
        "        text = parts[1]\n",
        "        audio_path = os.path.join(raw_audio_dir, filename)\n",
        "        \n",
        "        if not os.path.exists(audio_path):\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            wav, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "            wav_tensor = torch.from_numpy(wav).float().unsqueeze(0).unsqueeze(0)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                codes = codec.encode_code(wav_tensor)\n",
        "                codes = codes.squeeze(0).squeeze(0).cpu().numpy().flatten().tolist()\n",
        "                codes = [int(x) for x in codes]\n",
        "            \n",
        "            # Validate\n",
        "            if not codes or not all(0 <= c < 65536 for c in codes):\n",
        "                print(f\"ü¶ú Invalid codes: {filename}\")\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "            \n",
        "            codes_json = json.dumps(codes)\n",
        "            lines_to_write.append(f\"{filename}|{text}|{codes_json}\\n\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"ü¶ú L·ªói {filename}: {e}\")\n",
        "            skipped_count += 1\n",
        "    \n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.writelines(lines_to_write)\n",
        "    \n",
        "    print(f\"\\nü¶ú Ho√†n t·∫•t! ƒê√£ encode {len(lines_to_write)} m·∫´u\")\n",
        "    print(f\"   - L∆∞u t·∫°i: {output_path}\")\n",
        "    print(f\"   - B·ªè qua: {skipped_count}\")\n",
        "    return output_path\n",
        "\n",
        "encoded_metadata_path = encode_dataset(dataset_dir=\"dataset\", max_samples=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ 6. Setup Training\n",
        "\n",
        "C·∫•u h√¨nh LoRA v√† Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# LoRA Config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Training Config\n",
        "training_config = {\n",
        "    'model': \"pnnbao-ump/VieNeu-TTS-0.3B\",\n",
        "    'run_name': \"VieNeu-TTS-LoRA\",\n",
        "    'output_dir': \"output\",\n",
        "    'per_device_train_batch_size': 2,\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'learning_rate': 2e-4,\n",
        "    'max_steps': 5000,  # Gi·∫£m ƒë·ªÉ test nhanh\n",
        "    'logging_steps': 50,\n",
        "    'save_steps': 500,\n",
        "    'eval_steps': 500,\n",
        "    'warmup_ratio': 0.05,\n",
        "    'bf16': True,\n",
        "}\n",
        "\n",
        "def get_training_args(config):\n",
        "    return TrainingArguments(\n",
        "        output_dir=os.path.join(config['output_dir'], config['run_name']),\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        max_steps=config['max_steps'],\n",
        "        per_device_train_batch_size=config['per_device_train_batch_size'],\n",
        "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
        "        learning_rate=config['learning_rate'],\n",
        "        warmup_ratio=config['warmup_ratio'],\n",
        "        bf16=config['bf16'],\n",
        "        logging_steps=config['logging_steps'],\n",
        "        save_steps=config['save_steps'],\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=config['eval_steps'],\n",
        "        save_strategy=\"steps\",\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        report_to=\"none\",\n",
        "        dataloader_num_workers=2,  # Gi·∫£m ƒë·ªÉ tr√°nh l·ªói\n",
        "        ddp_find_unused_parameters=False,\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Training config ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 7. Dataset Class & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "def preprocess_sample(sample, tokenizer, max_len=2048):\n",
        "    speech_gen_start = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_START|>')\n",
        "    ignore_index = -100\n",
        "    \n",
        "    phones = sample[\"phones\"]\n",
        "    vq_codes = sample[\"codes\"]\n",
        "    \n",
        "    codes_str = \"\".join([f\"<|speech_{i}|>\" for i in vq_codes])\n",
        "    chat = f\"\"\"user: Convert the text to speech:<|TEXT_PROMPT_START|>{phones}<|TEXT_PROMPT_END|>\\nassistant:<|SPEECH_GENERATION_START|>{codes_str}<|SPEECH_GENERATION_END|>\"\"\"\n",
        "    \n",
        "    ids = tokenizer.encode(chat)\n",
        "    \n",
        "    # Pad/truncate\n",
        "    if len(ids) < max_len:\n",
        "        ids = ids + [tokenizer.pad_token_id] * (max_len - len(ids))\n",
        "    elif len(ids) > max_len:\n",
        "        ids = ids[:max_len]\n",
        "    \n",
        "    input_ids = torch.tensor(ids, dtype=torch.long)\n",
        "    labels = torch.full_like(input_ids, ignore_index)\n",
        "    \n",
        "    # Mask labels before speech generation\n",
        "    speech_gen_start_idx = (input_ids == speech_gen_start).nonzero(as_tuple=True)[0]\n",
        "    if len(speech_gen_start_idx) > 0:\n",
        "        speech_gen_start_idx = speech_gen_start_idx[0]\n",
        "        labels[speech_gen_start_idx:] = input_ids[speech_gen_start_idx:]\n",
        "    \n",
        "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"labels\": labels,\n",
        "        \"attention_mask\": attention_mask\n",
        "    }\n",
        "\n",
        "class VieNeuDataset(Dataset):\n",
        "    def __init__(self, metadata_path, tokenizer, max_len=2048):\n",
        "        self.samples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "        if not os.path.exists(metadata_path):\n",
        "            raise FileNotFoundError(f\"Missing: {metadata_path}\")\n",
        "        \n",
        "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('|')\n",
        "                if len(parts) >= 3:\n",
        "                    self.samples.append({\n",
        "                        \"filename\": parts[0],\n",
        "                        \"text\": parts[1],\n",
        "                        \"codes\": json.loads(parts[2])\n",
        "                    })\n",
        "        \n",
        "        print(f\"ü¶ú Loaded {len(self.samples)} samples from {metadata_path}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        text = sample[\"text\"]\n",
        "        \n",
        "        try:\n",
        "            phones = phonemize_with_dict(text)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Phonemization error: {e}\")\n",
        "            phones = text\n",
        "        \n",
        "        data_item = {\"phones\": phones, \"codes\": sample[\"codes\"]}\n",
        "        return preprocess_sample(data_item, self.tokenizer, self.max_len)\n",
        "\n",
        "print(\"‚úÖ Dataset class ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ 8. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, default_data_collator\n",
        "\n",
        "model_name = training_config['model']\n",
        "print(f\"ü¶ú Loading model: {model_name}\")\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load Dataset\n",
        "dataset_path = encoded_metadata_path  # From earlier step\n",
        "full_dataset = VieNeuDataset(dataset_path, tokenizer)\n",
        "\n",
        "# Train/Eval split (5%)\n",
        "val_size = max(1, int(0.05 * len(full_dataset)))\n",
        "train_size = len(full_dataset) - val_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"ü¶ú Train: {len(train_dataset)} | Eval: {len(eval_dataset)}\")\n",
        "\n",
        "# Apply LoRA\n",
        "print(\"ü¶ú Applying LoRA...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Trainer\n",
        "args = get_training_args(training_config)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "print(\"ü¶ú Starting training! (Good luck)\")\n",
        "trainer.train()\n",
        "\n",
        "# Save\n",
        "save_path = os.path.join(training_config['output_dir'], training_config['run_name'])\n",
        "print(f\"ü¶ú Saving model to: {save_path}\")\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(\"‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü¶ú Done!\n",
        "\n",
        "Model ƒë√£ ƒë∆∞·ª£c fine-tune v√† l∆∞u t·∫°i `output/VieNeu-TTS-LoRA/`.\n",
        "\n",
        "B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng checkpoint n√†y ƒë·ªÉ:\n",
        "- Inference / generate speech\n",
        "- Merge LoRA v√†o model g·ªëc\n",
        "- Ti·∫øp t·ª•c fine-tuning v·ªõi dataset kh√°c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import (\n",
        "    HfApi,\n",
        "    create_repo,\n",
        "    upload_folder\n",
        ")\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "HF_USERNAME = \"pnnbao-ump\"  # ‚ö†Ô∏è ƒë·ªïi\n",
        "REPO_NAME = \"VieNeu-TTS-0.3B-lora-ngoc-huyen\"\n",
        "LOCAL_LORA_DIR = \"output/VieNeu-TTS-LoRA\"\n",
        "BASE_MODEL = \"pnnbao-ump/VieNeu-TTS-0.3B\"\n",
        "DATASET_URL = \"https://huggingface.co/datasets/pnnbao-ump/ngochuyen_voice\"\n",
        "\n",
        "# ===================== README =====================\n",
        "README_CONTENT = f\"\"\"\n",
        "---\n",
        "language: vi\n",
        "license: cc-by-nc-4.0\n",
        "base_model: {BASE_MODEL}\n",
        "library_name: peft\n",
        "tags:\n",
        "  - lora\n",
        "  - text-to-speech\n",
        "  - tts\n",
        "  - vietnamese\n",
        "  - vieneu-tts\n",
        "---\n",
        "\n",
        "# ü¶ú VieNeu-TTS-LoRA (Ng·ªçc Huy·ªÅn)\n",
        "\n",
        "LoRA adapter ƒë∆∞·ª£c fine-tune t·ª´ base model **VieNeu-TTS-0.3B**\n",
        "ƒë·ªÉ hu·∫•n luy·ªán gi·ªçng ƒë·ªçc **Ng·ªçc Huy·ªÅn (Vbee)**.  \n",
        "\n",
        "Code finetune VieNeu-TTS t·∫°i repo: https://github.com/pnnbao97/VieNeu-TTS\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Base Model\n",
        "- Base model: `{BASE_MODEL}`\n",
        "- Repo n√†y **ch·ªâ ch·ª©a LoRA adapter**, kh√¥ng bao g·ªìm model g·ªëc.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Dataset\n",
        "- {DATASET_URL}\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"{BASE_MODEL}\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"{HF_USERNAME}/{REPO_NAME}\"\n",
        ")\n",
        "\n",
        "## Credits\n",
        "\n",
        "Base model: Ph·∫°m Nguy·ªÖn Ng·ªçc B·∫£o\n",
        "LORA finetuning: Ph·∫°m Nguy·ªÖn Ng·ªçc B·∫£o\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxx\") # Huggingface Token c·ªßa b·∫°n - ƒë·∫£m b·∫£o c√≥ quy·ªÅn write"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶ú Creating repo: pnnbao-ump/VieNeu-TTS-0.3B-lora-ngoc-huyen\n",
            "ü¶ú Uploading LoRA adapter to Hugging Face...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63ee193705684d0aa3c4a17db24bcf98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27849edd9e414fa89e2cb9b53d8d39a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33925823201345fb9ccfa6547ae8840b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...-LoRA/checkpoint-4500/rng_state.pth: 100%|##########| 14.6kB / 14.6kB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec3fd95e180a4132a10899b2d4f01c9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...S-LoRA/checkpoint-4500/scheduler.pt: 100%|##########| 1.47kB / 1.47kB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "341fbe37ecce40a6826c32524fff66c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...A/checkpoint-4500/training_args.bin: 100%|##########| 5.78kB / 5.78kB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77a0e5717b14442a924ef9cb90e997cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...-LoRA/checkpoint-5000/rng_state.pth: 100%|##########| 14.6kB / 14.6kB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f451d1d0deb34c89bebc84f4266d5695",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...S-LoRA/checkpoint-5000/scheduler.pt: 100%|##########| 1.47kB / 1.47kB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50b8fc95c59e4a05be25934fcb85216c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...A/checkpoint-5000/training_args.bin: 100%|##########| 5.78kB / 5.78kB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3752099af1594cef9053e531044a735f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...-TTS-LoRA/adapter_model.safetensors: 100%|##########| 12.8MB / 12.8MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a3fdfc281fb415a8f62fa5de59d4b2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...oint-5000/adapter_model.safetensors: 100%|##########| 12.8MB / 12.8MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "481b03fe19b14ceb960d4d60e4840617",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...S-LoRA/checkpoint-4500/optimizer.pt: 100%|##########| 25.8MB / 25.8MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7131071559f04a92a663a0212906bcf3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...S-LoRA/checkpoint-5000/optimizer.pt: 100%|##########| 25.8MB / 25.8MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Upload completed successfully!\n",
            "üîó https://huggingface.co/pnnbao-ump/VieNeu-TTS-0.3B-lora-ngoc-huyen\n"
          ]
        }
      ],
      "source": [
        "repo_id = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
        "print(f\"ü¶ú Creating repo: {repo_id}\")\n",
        "create_repo(\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True\n",
        ")\n",
        "\n",
        "# Write README.md\n",
        "readme_path = os.path.join(LOCAL_LORA_DIR, \"README.md\")\n",
        "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(README_CONTENT.strip())\n",
        "\n",
        "print(\"ü¶ú Uploading LoRA adapter to Hugging Face...\")\n",
        "upload_folder(\n",
        "    folder_path=LOCAL_LORA_DIR,\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Upload VieNeu-TTS LoRA adapter\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Upload completed successfully!\")\n",
        "print(f\"üîó https://huggingface.co/{repo_id}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
